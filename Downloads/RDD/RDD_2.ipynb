{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfgPiOklBTp7",
        "outputId": "3463c8ec-39b7-4502-8a3d-99e467747108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas openpyxl pyspark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7822a296"
      },
      "source": [
        "What are the first 5 records in the internship dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eba53f7a",
        "outputId": "6bbf0fbf-3d6d-40bd-dfe1-a6e0fc2ca0e4"
      },
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "import pandas as pd\n",
        "\n",
        "# Use existing SparkContext if available, otherwise create a new one\n",
        "sc = SparkContext.getOrCreate()\n",
        "sc.setLogLevel(\"ERROR\")\n",
        "\n",
        "# Read Excel using pandas\n",
        "df = pd.read_excel(\"internships.xlsx\")\n",
        "\n",
        "# Convert to list of rows\n",
        "data_list = df.values.tolist()\n",
        "\n",
        "# Create RDD\n",
        "rdd = sc.parallelize(data_list)\n",
        "\n",
        "print(\"First 5 records:\")\n",
        "for record in rdd.take(5):\n",
        "    print(record)\n",
        "\n",
        "# Note: sc.stop() is removed to allow reusing the SparkContext in other cells"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 records:\n",
            "['Java Development', 'SunbaseData', 'Work From Home', 'Java, OOP, Spring, Problem Solving', 'Above 90%', 'Above 70%', 'Above 9.0']\n",
            "['Accounting and Finance', 'DAKSM & Co. LLP', 'Noida', 'Accounting, Finance, MS Excel, Tally', 'Above 75%', 'Above 80%', 'Above 9.0']\n",
            "['Sales & Digital Marketing', 'Bharat Natural Elements Private Limited', 'Bangalore', 'Sales, Digital Marketing, SEO, Communication', 'Above 80%', 'Above 70%', 'Above 8.5']\n",
            "['Social Entrepreneurship', 'Hamari Pahchan NGO', 'Work From Home', 'NGO Management, Fundraising, Leadership', 'Above 90%', 'Above 90%', 'Above 9.0']\n",
            "['Videography & Photography', 'Esquare Lifestyle', 'Bangalore', 'Videography, Photography, Editing, Creativity', 'Above 70%', 'Above 80%', 'Above 7.5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14ea81d6"
      },
      "source": [
        "Which internship title has the highest number of openings?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f26c072",
        "outputId": "e8a802ac-3ab5-4cd8-cf6e-e66bdd92de4f"
      },
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "import pandas as pd\n",
        "\n",
        "# Use existing SparkContext if available\n",
        "sc = SparkContext.getOrCreate()\n",
        "sc.setLogLevel(\"ERROR\")\n",
        "\n",
        "df = pd.read_excel(\"internships.xlsx\")\n",
        "data_rdd = sc.parallelize(df.values.tolist())\n",
        "\n",
        "# Internship title is column 0\n",
        "title_count = data_rdd.map(lambda x: (x[0], 1)).reduceByKey(lambda a, b: a + b)\n",
        "max_title = title_count.sortBy(lambda x: -x[1]).first()\n",
        "\n",
        "print(f\"Internship title with highest openings: {max_title[0]} ({max_title[1]} openings)\")\n",
        "\n",
        "# Note: sc.stop() is removed to allow reusing the SparkContext in other cells"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Internship title with highest openings: Human Resources (HR) (5 openings)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which internship title has the lowest number of openings?"
      ],
      "metadata": {
        "id": "sKHSgcnsG_32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "import pandas as pd\n",
        "\n",
        "conf = SparkConf().setAppName(\"Internship_Q3\").setMaster(\"local[*]\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "df = pd.read_excel(\"internships.xlsx\")\n",
        "data_rdd = sc.parallelize(df.values.tolist())\n",
        "\n",
        "# Company name is column 1\n",
        "company_count = data_rdd.map(lambda x: (x[1], 1)).reduceByKey(lambda a, b: a + b)\n",
        "min_company = company_count.sortBy(lambda x: x[1]).first()\n",
        "\n",
        "print(f\"Company with lowest internship openings: {min_company[0]} ({min_company[1]} openings)\")\n",
        "\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TO3FKPrDB54Z",
        "outputId": "d4dd11c5-5f18-44de-e752-7c0d131bc435"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Company with lowest internship openings: SunbaseData (1 openings)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the average cgpa by location ?"
      ],
      "metadata": {
        "id": "aXDz1wzPHH-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Use existing SparkContext if available, otherwise create a new one\n",
        "sc = SparkContext.getOrCreate()\n",
        "sc.setLogLevel(\"ERROR\")\n",
        "\n",
        "# Load the Excel file\n",
        "# Correcting the file path to the uploaded file\n",
        "df = pd.read_excel(\"internships.xlsx\")\n",
        "\n",
        "# Convert dataframe to RDD\n",
        "data_rdd = sc.parallelize(df.values.tolist())\n",
        "\n",
        "# Helper function to extract numeric CGPA from string\n",
        "def extract_cgpa(value):\n",
        "    try:\n",
        "        # Use regex to get float number from string like \"Above 9.0\"\n",
        "        match = re.search(r\"\\d+(\\.\\d+)?\", str(value))\n",
        "        if match:\n",
        "            return float(match.group())\n",
        "        else:\n",
        "            return None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Map location and CGPA, filtering invalid entries\n",
        "location_cgpa = data_rdd.map(lambda x: (x[2], extract_cgpa(x[6]))) \\\n",
        "                        .filter(lambda x: x[1] is not None)\n",
        "\n",
        "# Calculate average CGPA by location\n",
        "avg_cgpa = location_cgpa.combineByKey(\n",
        "    lambda value: (value, 1),\n",
        "    lambda acc, value: (acc[0] + value, acc[1] + 1),\n",
        "    lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
        ").mapValues(lambda x: round(x[0] / x[1], 2))\n",
        "\n",
        "# Print results\n",
        "print(\"Average CGPA by location:\")\n",
        "for loc, avg in avg_cgpa.collect():\n",
        "    print(f\"{loc}: {avg}\")\n",
        "\n",
        "# Note: sc.stop() is removed to allow reusing the SparkContext in other cells"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9-YLb3kDp5H",
        "outputId": "f1b39ff2-20d7-4611-caa9-3710ebcafedd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average CGPA by location:\n",
            "Noida: 7.5\n",
            "Delhi: 7.4\n",
            "Mumbai: 7.0\n",
            "Hyderabad: 7.5\n",
            "Sachin INA: 6.5\n",
            "Work From Home: 7.5\n",
            "Bangalore: 7.79\n",
            "Gurgaon: 7.5\n",
            "Chennai: 6.0\n",
            "Chandigarh: 8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probability distribution of CGPA ranges ?"
      ],
      "metadata": {
        "id": "AQzuY6KJHUDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Use existing SparkContext if available\n",
        "sc = SparkContext.getOrCreate()\n",
        "sc.setLogLevel(\"ERROR\")\n",
        "\n",
        "df = pd.read_excel(\"internships.xlsx\")\n",
        "data_rdd = sc.parallelize(df.values.tolist())\n",
        "\n",
        "# Helper function to extract numeric CGPA from string\n",
        "def extract_cgpa(value):\n",
        "    try:\n",
        "        # Use regex to get float number from string like \"Above 9.0\"\n",
        "        match = re.search(r\"\\d+(\\.\\d+)?\", str(value))\n",
        "        if match:\n",
        "            return float(match.group())\n",
        "        else:\n",
        "            return None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# CGPA is column 6\n",
        "# Map to extract CGPA and filter out invalid entries\n",
        "cgpa_rdd = data_rdd.map(lambda x: extract_cgpa(x[6])).filter(lambda x: x is not None)\n",
        "\n",
        "total = cgpa_rdd.count()\n",
        "above_8 = cgpa_rdd.filter(lambda x: x > 8.0).count()\n",
        "between_6_8 = cgpa_rdd.filter(lambda x: 6.0 <= x <= 8.0).count()\n",
        "below_6 = cgpa_rdd.filter(lambda x: x < 6.0).count()\n",
        "\n",
        "# Calculate probabilities, handling the case where total is 0\n",
        "p_above_8 = round(above_8 / total, 3) if total > 0 else 0\n",
        "p_between_6_8 = round(between_6_8 / total, 3) if total > 0 else 0\n",
        "p_below_6 = round(below_6 / total, 3) if total > 0 else 0\n",
        "\n",
        "print(\"Probability distribution:\")\n",
        "print(f\"P(CGPA > 8.0)     = {p_above_8}\")\n",
        "print(f\"P(6.0 ≤ CGPA ≤ 8.0) = {p_between_6_8}\")\n",
        "print(f\"P(CGPA < 6.0)     = {p_below_6}\")\n",
        "\n",
        "# Note: sc.stop() is removed to allow reusing the SparkContext in other cells\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vshLVBAAF6Dy",
        "outputId": "e537b37b-3b70-429c-a1fa-89406b2133e9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability distribution:\n",
            "P(CGPA > 8.0)     = 0.265\n",
            "P(6.0 ≤ CGPA ≤ 8.0) = 0.735\n",
            "P(CGPA < 6.0)     = 0.0\n"
          ]
        }
      ]
    }
  ]
}